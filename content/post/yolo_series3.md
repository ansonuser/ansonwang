---
author: "Anson Wang"
date: "2025-05-27"
description: "Introduction of yolo family"
title: "Yolo Family (3)"
tags: [
    "Yolo",
    "Machine Learning",
    "Computer Vision",
    "Model Compression"
]
categories: ["YOLO", "Model Compression"]
math: true
---


## Review

In past research, many lightwieght/efficient designs have been studied. The core philosiphy is learning efficiently with less parameters.  
Here are some famous strategies:

1. **Depthwise separable convolution:**

    Given an input with size(c, h, w) of **(3, 64, 64)** and a convolution(window size=3) operation turns it to **(4, 64, 64)** requires **(3, 3, 3, 4)** parameters. It slides a cubic size of **(3, 3, 3)** over the input 
    4 times (See Fig 1). This operation provides transformation of both channel and 2d embeddings. Is there a cheaper way to do so ? "Combination of depthwise and pointwise share same philosiphy in physic. "
    
    First use **(1, 3, 3)** on each channel(size=3) for 2d embedding transformation and followed by **(4, 1, 1)** convolution for channel transformation (See Fig 2). In general, turn size of input **(c1, h1, w1)** to output size **(c2, h1, w1)** with window size **s1** requires **c1 x c2 x s1 x s1** parameters in normal case. With depthwise and pointwise, it takes **c1 x s1 x s1 + c2 x 1 x 1** and **c2** is usually larger than **c1**,**s1** saving ratio converges to **1/(c1 x s1 x s1)** as **c2** goes to infinity.        

{{< figure src="/ansonwang/images/normal_conv.png" caption="Fig1. Normal Convolution (src: Blog of CSDN)" >}}

<!-- <p> src: [Blog of CSDN](https://blog.csdn.net/zhw864680355/article/details/105453945) </p>  -->
---

{{< figure src="/ansonwang/images/depthwise_pointwise.png" caption="Fig2. Depthwise separable (src: Post of Youssef Hosni)" >}}
<!-- (https://www.linkedin.com/posts/youssef-hosni-b2960b135_q115-what-is-a-depthwise-separable-layer-activity-7038908659786989568-Nn7y/) -->



2. **GhostModule:**

    The researcher found some similar image representation in different channels, called ghost effect(see Fig3). 
    Here comes two ideas:

    - No need to use all features
    - Do some transformations on similar features. 
    
    Though GhostModule doesn't really solve the problem, the module gives a general solution to make model learn diversely. 
    It doesn't specifically do transformation on similiar features rather use uniform way to alleviate the issue. 
    During the convolution, all channels apply **(1, 1, 1)** convolution for slight change. Hold half(can be any size) of channels still and concate with the other half which has been passed to some cheap operations (linear, relu, depthwise conv, etc) to reduce ghost effect(see Fig4). CSPNet develops based on this idea. 

{{< figure src="/ansonwang/images/ghost_effect.png" caption="Fig3. Visualization of some feature maps generated by the first residual group in ResNet-50. (src: Kai Han, 2020 )" >}}

---

{{< figure src="/ansonwang/images/ghost_module.png" caption="Fig4. Ghost module. phi represents the cheap operation. (src: Kai Han, 2020 )" >}}


3. **Group convolution with shuffling:** 

    Use shuffle to offset the side effect of using same parameters for one group. It reduce parameters size to **1/num_of_group**. (see Fig5).

{{< figure src="/ansonwang/images/group_conv.png" caption="Fig5. Group convolution. (src: Blog of CSDN)" >}}

<!-- (https://blog.csdn.net/chen1234520nnn/article/details/119931458) -->

4. **NAS approach:**

    Use RNN-wise to create different model structures and evaluate the performance to find the best structure in defined domain. (see Fig6.)

{{< figure src="/ansonwang/images/nasnet.png" caption="Fig6. Architecture to construct blocks of model. (src: Barret Zoph, 2018)">}}

5. **EfficientNet:**
 
    Studied compound scaling for depth, width and resolution at the same time which provides a framework how to scale a neural network.(see Fig7.)

{{< figure src="/ansonwang/images/efficientnet.png" caption="Fig7. Design of the model and efficientnet shows compound (e) gives the best result. (src: Mingxing Tan, 2020)">}}

6. **RevCol:**

    Do not use irreversible(relu, max-pooling, etc) functions avoid memorizing output in each layer and the network can go much deeper cheaply. 
    
    Consider a composite function **f = g1(g2(g3(....gk(.))))**. 
    For **f(x) = y**, simply reverse functions step by step gives intermediate output of each layer.  

Besides the model design, some existed defects have been analyzed, such as loss, activated function and attention mechanism.

1. **Generalized focal loss:**

    Two issues have been discovered here. First, IOU and bounding box are decoupled during the training but get coupled in inference(like a indicate) make training space and testing space inconsistent. Second, the boundary of objects are not a point rather more like a distribition diffuses from the spike. 
    
    To solve these issues, they consider IOU and bounding box loss at the same time, named quality focal loss. Another loss treats points of bounding box as distribution. Bucket the values into histogram bins and estimate the value with $$\sum_{i=1}^n p_i*i $$ where n is number of bins. 

    {{< figure src="/ansonwang/images/boundary_issue_dfl.png" caption="Fig8. Boundary is hard to set by a point. (src: Xiang Lin, 2020)">}}

    --- 

    {{< figure src="/ansonwang/images/gfl.png" caption="Fig9. General focal loss includes QFL and DFL. (src: Xiang Lin, 2020)">}}


2. **Multi-scale learning:**

    Featrue Pyramid wise approach makes connection of different scale of features. Some approaches directly predict based on these features and learn from prediction error. 

3. **Swish / H-swish:**

    $$ swish_\beta(x) $$ is defined as $$ x \cdot signoid(\beta x) $$ which decrease issue of gradient vanishing or explode. ($$ \beta $$ can be constant or trainable). Hard swish is a simplified version to approach swish (see below)


    H-swish(x)$$  = 0\ \ if\ x \leq -3 $$

    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = x\ \ if\ x \geq 3  $$

    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =\ x\cdot(x + 3) / 6 ;\ O.W.$$

4. **Spatial and channel attention:**

    Shuffle attention module combines spatial-wise attention (split features and aggregate) and channel-wise attention.(See Fig10.) 


    {{< figure src="/ansonwang/images/SA_Net.png" caption="Fig10. Design of the model and efficientnet shows compound(e) gives the best result. (src: Qing-Long Zhang, 2021)">}}

## YOLOV8

V8 wasn't published but released by ultralytics on github. 

This version includes DFL, replaces resblock with ghost shuffle convolution in CSP block (called C2F block).

## YOLOV9

V9 introduces PGI(programmable gradient information) has two contributions. 

1. It gives  multi-level auxiliary information to reduce the problem of deep surpervise (deep supervise applys multi-level prediction leads that error from shallow layers can't be improved by deeper layers) 

2. Intergrate RevCol into framework make reversible branch detachable during inference.


{{< figure src="/ansonwang/images/pgi.png" caption="Fig11. Design of programmable gradient information. (src: Chien-Yao Wang, 2024)">}}

The authors also experiments with different structures on backbone (GELAN) to further boost
the performance. It shows the GELAN can retain quite complete information up to the 150th layers compares to ResNet, CSPNet and PlainNet.


{{< figure src="/ansonwang/images/v9_retention.png" caption="Fig12. Feature maps output of different structures. (src: Chien-Yao Wang, 2024)">}}


## YOLOV10

The team from Tsinghua University develops V10 based on V8. They propose a NMS-free design fixes the cost of postporcess. Though the predictions of classes and bounding boxes have been decoupled since YOLOV5 but head of class still take too many parameters(number of class is huge compare to dimension of bounding boxes) without contributing much to the performance. They adopt depthwise seperable convolution on the head of classes. Let's see how it solves the above issues:

- Dual label assignments:

    Though Hungarian-wise approach gives **1-1** mapping for the prediction to the target for each anchor. However, the model needs to learn step by step which implies that **1-many** is a more resonable way to learn. Therefore, the design inculdes two heads, one for **1-1** and another one for **1-many** roled as the teacher (consistent match metric) in this set-up. 

    The metric is defined as :

    $$ m(\alpha, \beta) = s\cdot p^{\alpha} IoU(\hat{b}, b)^{\beta} $$

    where p is the classification score and $$ \alpha, \beta $$ are hyperparameters
    
    $$ \hat{b}, b $$ denote the bounding box of preidciton and instance respectively,
    **s** reperesents the spatial prior indicating whether the anchor point of prediction is within the instance(just add one dimension to predict it).

    The target is to miniminze the loss between prediction from **1-many** and **1-1**. Given an instance, denote its largest IoU with prediction as u* and let $$ \Omega $$ be a set of targets which have non-zero intersection with that largest prediction. Derive 
    
    $$ t_{o2m,j}=u^{\*} \frac{m_{o2m,j}}{m^{\*}_{o2m}} $$ for $$ j \in \Omega, m^{\*}$$ denotes largest similarity

    and $$ t_{o2o,i}=u^{\*} $$. 
    
    The 1-Wasserstein distance between **1-1** and **1-many** can be defined as:

    $$ D = t_{o2o,i} - I(i \in \Omega)t_{o2m,i} + \sum_{k \in \Omega/\{i\}}{t_{o2m,k}} $$ 

    see schematic in Figure 13 for better understanding
  
  {{< figure src="/ansonwang/images/dual_label_assignment.png" caption="Fig13. Structure od dual label assignment (src: Ao Wang, 2024)">}}

    



- Model design philosophy: 

    By inspection of the rank in each layer, they found the later stage shows redundant. Further inspect the performance variation of replacing the basic block in the leading stage with **CIB** (depthwise separable convolution in CSP). If there is no performance degradation proceed with the replacement of the next stage and halt the process otherwise. Consequently, achieving higher efficiency without compromising performance. 
    
    Besides, small scaled models need to use **7x7** depthwise convolution for sufficient receptive field, which expands as the model size increase **3x3** is sufficient for larger models. Attention mechanism is also intergrated into the module.


    {{< figure src="/ansonwang/images/yolov10_model_design.png" caption="Fig14. Analysis of rank across stages and design of block (src: Ao Wang, 2024)">}}


- Rank diminishing:

    In forward pass, the composite function starts from the first layer and keeps multipling all the way to the end of model and algebra tells us 
    $$ Rank(W_{d}) = min_{i \leq d } Rank(W_i) $$

    For single layer analysis, simply apply SVD decomposition. Effective rank can be set as a threshold, $$ \epsilon \cdot \sigma_1 $$ 
    
    where $$ \sigma_1 $$ is the largest eigenvalue and $$ \epsilon \in \(0, 1\)$$

## References

[1] Andrew G. Howard, et al. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, CVPR, 2017

[2] Qing-Long Zhang, et al. SA-NET: SHUFFLE ATTENTION FOR DEEPCONVOLUTIONAL NEURAL NETWORKS, CVPR 2021

[3] Xiang Li, et al, Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection, CVPR, 2020

[4] Y Cai, et al. Reversible Column Networks, ICLR 2023

[5] Y Ioannou, et al. Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups, CVPR, 2016

[6] Barret Zoph, et al. Learning Transferable Architectures for Scalable Image Recognition, CVPR, 2018

[7] Kai Han, et al. GhostNet: More Features from Cheap Operations, CVPR, 2020

[8] Francois Chollet, et al. Xception: Deep Learning With Depthwise Separable Convolutions, CVPR, 2017 

[9] Muhammad Yaseen, What is YOLOv8: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector, CVPR, 2024

[10] Chien-Yao Wang, et al. YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information, CVPR, 2024

[11] Ruili Feng, et al. Rank Diminishing in Deep Neural Networks, 2022

[12] Ao Wang, et al. YOLOv10: Real-Time End-to-End Object Detection, CVPR, 2024
